{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeQ1hQEo450ejG21wy7Yr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratik94229/NLP/blob/main/Frequency_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvi3zUNP3sxk",
        "outputId": "075eb5ae-06bd-4c66-92f3-7db7af7c6e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "######################################################################################\n",
        "# THis example is pretty much entirely based on this excellent blog post\n",
        "# http://glowingpython.blogspot.in/2014/09/text-summarization-with-nltk.html\n",
        "# Thanks to TheGlowingPython, the good soul that wrote this excellent article!\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "#  we will use 2 functions from nltk\n",
        "#  sent_tokenize: given a group of text, tokenize (split) it into sentences\n",
        "#  word_tokenize: given a group of text, tokenize (split) it into words\n",
        "#  stopwords.words('english') to find and ignored very common words ('I', 'the',...) \n",
        "######################################################################################\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "######################################################################################\n",
        "# defaultdict is class that inherits from dictionary, but has\n",
        "# one additional nice feature: Usually, a Python dictionary throws a KeyError if you try \n",
        "# to get an item with a key that is not currently in the dictionary. \n",
        "# The defaultdict in contrast will simply create any items that you try to access \n",
        "# (provided of course they do not exist yet). To create such a \"default\" item, it relies \n",
        "# a function that is passed in..more below. \n",
        "######################################################################################\n",
        "from collections import defaultdict\n",
        "\n",
        "######################################################################################\n",
        "#  punctuation to ignore punctuation symbols\n",
        "######################################################################################\n",
        "from string import punctuation\n",
        "\n",
        "######################################################################################\n",
        "# heapq.nlargest is a function that given a list, easily and quickly returns\n",
        "# the 'n' largest elements in the list. More below\n",
        "######################################################################################\n",
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqVt_it5mBzM",
        "outputId": "0d506f79-8ebb-4168-c559-2b5369b90745"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################################\n",
        "# Our first class, named FrequencySummarizer \n",
        "######################################################################################\n",
        "class FrequencySummarizer:\n",
        "    # Constructer\n",
        "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
        "        self._min_cut = min_cut\n",
        "        self._max_cut = max_cut \n",
        "        # Words that have a frequency term lower than min_cut \n",
        "        # or higer than max_cut will be ignored.\n",
        "\n",
        "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
        "        # Punctuation symbols and stopwords (common words like 'an','the' etc) are ignored\n",
        "        \n",
        "        # Here self._min_cut, self._max_cut and self._stopwords are created as member variables\n",
        "        # i.e. each object (instance) of this class will have an independent version of these\n",
        "        # variables. \n",
        "\n",
        "   \n",
        "\n",
        "    def _compute_frequencies(self, word_sent):\n",
        "      '''\n",
        "        next method (member function) which takes in self (the special keyword for this same object)\n",
        "        as well as a list of sentences, and outputs a dictionary, where the keys are words, and\n",
        "        values are the frequencies of those words in the set of sentences'''\n",
        "      freq = defaultdict(int)\n",
        "        # defaultdict, which we referred to above - is a class that inherits from dictionary,\n",
        "        # with one difference: Usually, a Python dictionary throws a KeyError if you try \n",
        "        # to get an item with a key that is not currently in the dictionary. \n",
        "        # The defaultdict in contrast will simply create any items that you try to access \n",
        "        # (provided of course they do not exist yet). THe 'int' passed in as argument tells\n",
        "        # the defaultdict object to create a default value of 0\n",
        "\n",
        "      #Calculating frequency of each words and storing it in default dictionary \n",
        "      for sen in word_sent:\n",
        "        # looping through sentence\n",
        "          for word in sen:\n",
        "            # looping through words in sentence\n",
        "            if word not in self._stopwords:\n",
        "                # if the word is in the member variable (dictionary) self._stopwords, then ignore it,\n",
        "                # else increment the frequency. Had the dictionary freq been a regular dictionary (not a \n",
        "                # defaultdict, we would have had to first check whether this word is in the dict\n",
        "                freq[word] += 1\n",
        "\n",
        "      '''\n",
        "      Now we will go through our frequency list and do 2 things\n",
        "      normalize the frequencies by dividing each by the highest frequency (this allows us to \n",
        "      always have frequencies between 0 and 1, which makes comparing them easy and then\n",
        "      filter out frequencies that are too high or too low.This would help us get better results.\n",
        "\n",
        "      '''\n",
        "      m = float(max(freq.values()))\n",
        "      # getting the highest frequency of any word in the list of words\n",
        "        \n",
        "      for w in list(freq.keys()):\n",
        "    \n",
        "        freq[w] = freq[w]/m\n",
        "        # divide each frequency by that max value, so it is now between 0 and 1 and updating it in original dictionary\n",
        "\n",
        "        if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
        "        # really common or really uncommon. In either case - delete it from our dictionary\n",
        "         del freq[w]\n",
        "\n",
        "         return freq\n",
        "    \n",
        "    #Function to rank sentences based on their importance(i.e frequency of words )\n",
        "    def summarize(self, text, n):\n",
        "        '''\n",
        "        next method (member function) which takes in self (the special keyword for this same object)\n",
        "        as well as the raw text, and the number of sentences we wish the summary to contain. Return the \n",
        "        summary of the document\n",
        "        '''\n",
        "        # split the text into sentences\n",
        "        sents = sent_tokenize(text)\n",
        "       \n",
        "        # assert is a way of making sure a condition holds true, else an exception is thrown. Used to do \n",
        "        # sanity checks like making sure the summary is shorter than the original article.\n",
        "        assert n <= len(sents)\n",
        "\n",
        "        # converting each sentence to lower-case, then \n",
        "        # splits each sentence into words, then takes all of those lists (1 per sentence)\n",
        "        # and packs them into bigger list\n",
        "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
        "        \n",
        "        # make a call to the method (member function) _compute_frequencies, and places that in\n",
        "        # the member variable _freq.\n",
        "        self._freq = self._compute_frequencies(word_sent)\n",
        "\n",
        "        # creating an empty dictionary (of the superior defaultdict variety) to hold the rankings of the \n",
        "        # sentences.  \n",
        "        ranking = defaultdict(int)\n",
        "        \n",
        "        for i,sent in enumerate(word_sent):\n",
        "          # enumerate(sequence) will return (0, thing[0]), (1, thing[1]), (2, thing[2]), and so forth.\n",
        "\n",
        "          # for each word in this sentence\n",
        "          for w in sent:\n",
        "            # if this is not a stopword (common word), add the frequency of that word \n",
        "            # to the weightage assigned to that sentence \n",
        "            if w in self._freq:\n",
        "              ranking[i] += self._freq[w]\n",
        "\n",
        "        # we want to return the first n sentences with highest ranking so we are using the nlargest function to do so\n",
        "        # this function needs to know how to get the list of values to rank, so give it a function - simply the \n",
        "        # get method of the dictionary\n",
        "        sents_idx = nlargest(n, ranking, key=ranking.get)\n",
        "        return [sents[sen] for sen in sents_idx]\n",
        "       # return a list with these values in a list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AGnKJ1nkI3T5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################################\n",
        "# Now to get a URL and summarize\n",
        "######################################################################################\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# This function takes in a URL as an argument, and returns only the text of the article in that URL.\n",
        "def get_only_text_washington_post_url(url):\n",
        "    # download the URL \n",
        "    page = urllib.request.urlopen(url).read().decode('utf8')\n",
        "\n",
        "    # initialise a BeautifulSoup object with the text of that URL\n",
        "    soup = BeautifulSoup(page)\n",
        "    \n",
        "\n",
        "    #to get everything in that text that lies between a pair of <article> and </article> tags.\n",
        "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
        "  \n",
        "\n",
        "    # Owe got everything between the <article> and </article> tags, but that everything\n",
        "    # includes a bunch of other stuff we don't want\n",
        "\n",
        "    # Now - repeat, but this time we will only take what lies between <p> and </p> tags\n",
        "    # these are HTML tags for \"paragraph\" i.e. this should give us the actual text of the article\n",
        "    soup2 = BeautifulSoup(text)\n",
        "\n",
        "    #to get everything in that text that lies between a pair of \n",
        "    # <p> and </p> tags.\n",
        "\n",
        "    text = ' '.join(map(lambda p: p.text, soup2.find_all('p')))\n",
        "\n",
        "    # Return a pair of values (article title, article body)\n",
        "    return soup.title.text, text\n",
        "\n"
      ],
      "metadata": {
        "id": "JZ27BhSVKARv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the article we would like to summarize\n",
        "someUrl = \"https://www.washingtonpost.com/technology/2023/05/02/ai-jobs-takeover-ibm/\"\n",
        "\n",
        "# get the title and text\n",
        "textOfUrl = get_only_text_washington_post_url(someUrl)\n",
        "\n",
        "\n",
        "# instantiate our FrequencySummarizer class and get an object of this class\n",
        "fs = FrequencySummarizer()\n",
        "\n",
        "# get a summary of this article that is 3 sentences long\n",
        "summary = fs.summarize(textOfUrl[1], 3)\n"
      ],
      "metadata": {
        "id": "jrHkwgI8vFOl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biXNpQGZvjJj",
        "outputId": "c6f1f6a6-0703-4674-e872-6d709dadbc48"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Goldman Sach’s March report predicted 18 percent of work worldwide could be computerized, with white-collar workers more at risk than manual laborers.The ability of AI software to generate new content that is “indistinguishable from human-created output” and breaks down communication barriers between humans and machines are key to why it might drastically affect the workforce, the report said.Using jobs data in both the United States and Europe, report writers found that roughly two-thirds of current jobs are exposed to some degree of AI automation, and that generative AI could substitute for up to one-fourth of current work done by humans.Your next job interview could be judged by AI.',\n",
              " '(Christopher Goodney/Bloomberg News)Listen4 minComment on this storyCommentGift ArticleShareIBM Corp. said it expects to pause hiring for jobs that artificial intelligence could do, indicating that the potentially groundbreaking technology is beginning to disrupt how humans work.Arvind Krishna, the company’s chief executive, estimated up to 7,800 jobs at the company could be affected, according to an interview with Bloomberg News on Monday, and hiring in back-office roles such as human resources could be at risk of being suspended or slowed down.Tech is not your friend.',\n",
              " 'After Chegg CEO Dan Rosensweig said in his company’s Monday evening earnings call that ChatGPT is slowing his customer-growth rate, the company’s stock fell 45 percent as the market opened Tuesday.Advertisement“This is not a sky’s falling thing,” Rosensweig said.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}